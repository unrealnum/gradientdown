# 决策树



[toc]

## 包含...

* #### 根节点——属性测试

* #### 内部节点——属性测试

* #### 叶节点——决策结果



## 构建过程



### 1. 分类

* #### 不能分类时返回

* #### 能分类：选择划分特征，进行划分，对未划分的子集进行

### 2. 划分选择



#### 2.1.1 信息增益

##### 定义：信息熵：

$$
Ent(D)==-\sum_{k=1}^{|y|}p_klog_2p_k
$$

##### 从而定义信息增益：

$$
Gain(D,a)=Ent(D)-\sum_{v=1}^{V}\frac{|D^v|}{D}Ent(D^v)
$$

**<font color='red'>用某个属性进行划分</font>**的信息增益越大，则意味着使用该属性来进行划分的的”纯度提升“越大（信息不确定性下降最多）



##### <font color='purple'>缺点：信息增益准则对可取值数量较大的属性有所偏好</font>



#### 2.1.2 增益率

$$
Gain\_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}
$$

$$
IV(a)=-\sum_{v=1}^{V}(\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|})
$$

##### $IV(a)$称为属性的固有值：属性的可能取值越多，IV(a)的值通常越大。（有效的处理了信息增益/信息熵的缺点）



##### <font color='purple'>缺点：信息增益准则对可取值数量较小的属性有所偏好</font>

##### （处理方法：找信息增益平均水平最高的，再用增益率找）



#### 2.1.3 基尼系数



##### 定义基尼值：

$$
Gini(D)=\sum_{k=1}^{|y|}\sum_{k'\neq k}p_k p_{k'}
=1-\sum_{k=1}^{|y|}p_k^2
$$

##### 反映了从数据集D中随机抽取两个样本，其类别标识不一致的概率



##### $Gini$越小，数据集纯度越高

##### 从而定义基尼系数：

$$
Gini\_index(D,a)=\sum_{v=1}^V\frac{|D^v|}{D}Gini(D^v)
$$

##### 选择划分后基尼系数最小的作为最优划分属性



### 3.剪枝处理



##### 决策树在进行构建时可能会出现决策划分分支过多，造成过拟合



#### 剪枝策略



##### 3.1.1：预剪枝

**<font color='red'>生成决策树时，若当前节点不能带来泛化性能的提升，则停止划分并将当前节点标记为叶子节点。</font>**

**<font color='purple'>缺点：基于贪心的决策树构造，可能出现欠拟合</font>**



##### 3.1.2：后剪枝

**<font color='red'>生成一棵决策树后，从底向上的对判断节点进行考察，若当前节点被替换后能提升泛化性能，则替换该节点为叶子节点。</font>**

##### 优点：较预剪枝欠拟合风险小

##### 缺点：训练时间长



##### 性能评估依据：见性能评估



## 多变量决策树



#### 分类边界太过复杂时，硬分类时空复杂度太大。

#### 那么，就用<font color='red'>属性的组合</font>作为决策依据

