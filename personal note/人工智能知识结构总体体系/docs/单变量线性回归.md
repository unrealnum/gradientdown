# 单变量线性回归



[toc]

## 如何拼凑一个单变量线性回归模型？

* Step 1：数据集——需要预测的数据（内含实例总数，特征/输入变量，目标变量/输出变量）
* Step 2：构建一个数据模型：我们假设h为算法的估计值

$$
h_{\theta}(x)=\theta_0+\theta_1x
$$

* Step 3：思考如何令代价函数变小？

$$
J（\theta_0,\theta_1）=\sum^m_1\frac{1}{2m}(h_{\theta}(x^{(i)})-y^{(i)})^2
$$

* Step 4：梯度下降？

  * 梯度下降背后的思想是：开始时我们随机选择一个参数的组合（θ~0~,θ~1~,...,θ~n~）

  * 计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。

  * 我们持续这么做直到到到 一个局部最小值（local minimum）

    * 因为我们并没有尝试完所有的参数组合
    * 所以不能确定 我们得到的局部最小值是否便是全局最小值（global minimum）
    * 选择不同的初始参数组合， 可能会找到不同的局部最小值。

  * $$
    \theta_j=\theta_j-\alpha\frac{\partial}{\partial \theta_j}J(\theta_0,\theta_1),j=0,1
    $$

  * $\alpha :$学习率

* Step 5：写算法

* Step 6：会不会线性回归更好？？

  * 线性回归